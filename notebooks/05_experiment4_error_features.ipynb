{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 4: Full Feature Set with Forecast Error Features\n",
        "\n",
        "This notebook extends Experiment 3 by adding forecast error/coverage features that measure model health over time.\n",
        "\n",
        "**New Features Added:**\n",
        "- Rolling MAE: How accurate have recent forecasts been?\n",
        "- Rolling Bias: Is the model systematically over/under-predicting?\n",
        "- Rolling Coverage: Are realized returns falling within prediction intervals?\n",
        "\n",
        "**Hypothesis:**\n",
        "When the forecast model is performing poorly (high error, low coverage), we should be less confident in predictions. These features let the tabular model learn when to trust the forecasts.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Run Experiment 0/1 first to generate labeled dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # Symbols\n",
        "    \"symbols_to_train\": [\"SPY\"],\n",
        "    \"context_symbols\": [\"SPY\"],\n",
        "    \n",
        "    # Data limits\n",
        "    \"max_rows_per_symbol\": 6500,\n",
        "    \n",
        "    # Label parameters\n",
        "    \"label_col\": \"label\",\n",
        "    \"vertical_barrier_bars\": 26,\n",
        "    \n",
        "    # Split parameters\n",
        "    \"embargo_bars\": 26,\n",
        "    \"tune_window\": 260,\n",
        "    \"test_window\": 520,\n",
        "    \"min_train_size\": 2000,\n",
        "    \n",
        "    # AutoGluon Tabular\n",
        "    \"time_limit_sec\": 1200,\n",
        "    \"presets\": \"best_quality\",\n",
        "    \n",
        "    # Time Series\n",
        "    \"ts_prediction_length\": 26,\n",
        "    \"ts_presets\": \"chronos_small\",\n",
        "    \"ts_train_lookback_years\": 5.0,\n",
        "    \n",
        "    # Feature parameters\n",
        "    \"feature_set\": \"small\",\n",
        "    \"forecast_prefix\": \"fc_\",\n",
        "    \"include_relative\": True,\n",
        "    \n",
        "    # Error feature parameters\n",
        "    \"error_prefix\": \"err_\",\n",
        "    \"mae_window\": 20,\n",
        "    \"bias_window\": 20,\n",
        "    \"coverage_window\": 50,\n",
        "    \n",
        "    # Reproducibility\n",
        "    \"random_seed\": 42,\n",
        "    \n",
        "    # Force options\n",
        "    \"force_data_refresh\": False,\n",
        "    \"force_ts_retrain\": False,\n",
        "    \"force_forecast_regenerate\": False,\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q autogluon.tabular[tabarena] || pip install -q autogluon.tabular[all]\n",
        "!pip install -q autogluon.timeseries[chronos-openvino]\n",
        "!pip install -q pandas numpy pyarrow scikit-learn pytz alpaca-py\n",
        "print(\"\\nInstallation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone/update repository\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/mh122333/ETF-Dual-Foundation-Project-CC-Version.git\"\n",
        "REPO_DIR = \"/content/ETF-Dual-Foundation-Project-CC-Version\"\n",
        "BRANCH = \"claude/build-pipeline-sanity-exp-iVs65\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    %cd {REPO_DIR}\n",
        "    !git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}\n",
        "else:\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "    %cd {REPO_DIR}\n",
        "    !git checkout {BRANCH}\n",
        "\n",
        "print(f\"\\nOn branch: {BRANCH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path and set random seeds\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "sys.path.insert(0, '/content/ETF-Dual-Foundation-Project-CC-Version/src')\n",
        "\n",
        "random.seed(CONFIG[\"random_seed\"])\n",
        "np.random.seed(CONFIG[\"random_seed\"])\n",
        "\n",
        "print(f\"Random seed set to: {CONFIG['random_seed']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from etf_pipeline.utils.paths import ensure_dirs, get_labeled_dataset_path\n",
        "from etf_pipeline.splits.purged_walkforward import (\n",
        "    create_single_split,\n",
        "    apply_split_to_dataframe,\n",
        "    validate_split_no_leakage,\n",
        ")\n",
        "from etf_pipeline.models.tabular_baseline import (\n",
        "    train_tabular_baseline,\n",
        "    predict_tabular,\n",
        ")\n",
        "from etf_pipeline.metrics.classification import (\n",
        "    compute_all_metrics,\n",
        "    save_metrics,\n",
        "    print_metrics_summary,\n",
        ")\n",
        "\n",
        "from etf_pipeline.timeseries.dataset import prepare_ts_training_data\n",
        "from etf_pipeline.timeseries.train import load_or_train_timeseries_predictor\n",
        "from etf_pipeline.timeseries.rolling_predict import load_or_generate_forecasts\n",
        "\n",
        "from etf_pipeline.features.forecast_features import (\n",
        "    merge_forecast_features,\n",
        "    get_forecast_feature_names,\n",
        "    FEATURE_SET_CONFIGS,\n",
        ")\n",
        "from etf_pipeline.features.context_features import add_context_features\n",
        "from etf_pipeline.features.forecast_error_features import (\n",
        "    compute_forecast_errors,\n",
        "    compute_rolling_error_features,\n",
        "    get_error_feature_names,\n",
        ")\n",
        "from etf_pipeline.features.baseline import get_feature_columns\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directories\n",
        "paths = ensure_dirs()\n",
        "\n",
        "run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "symbols_str = \"_\".join(CONFIG[\"symbols_to_train\"])\n",
        "RUN_ID = f\"exp4_{symbols_str}_{run_timestamp}\"\n",
        "print(f\"Run ID: {RUN_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load labeled dataset\n",
        "labeled_dataset_path = get_labeled_dataset_path()\n",
        "if not labeled_dataset_path.exists():\n",
        "    raise FileNotFoundError(f\"Labeled dataset not found. Run Experiment 0/1 first.\")\n",
        "\n",
        "full_df = pd.read_parquet(labeled_dataset_path)\n",
        "print(f\"Loaded {len(full_df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw bars\n",
        "bars_path = paths[\"raw\"] / \"bars_30min.parquet\"\n",
        "\n",
        "if bars_path.exists():\n",
        "    bars_df = pd.read_parquet(bars_path)\n",
        "else:\n",
        "    from google.colab import userdata\n",
        "    from alpaca.data.historical import StockHistoricalDataClient\n",
        "    import pytz\n",
        "    from etf_pipeline.data.alpaca import load_all_symbols\n",
        "    \n",
        "    api_key = userdata.get(\"PAPER_KEY\")\n",
        "    api_secret = userdata.get(\"PAPER_SEC\")\n",
        "    client = StockHistoricalDataClient(api_key, api_secret)\n",
        "    \n",
        "    eastern = pytz.timezone(\"US/Eastern\")\n",
        "    start = eastern.localize(datetime(2019, 1, 1))\n",
        "    end = eastern.localize(datetime(2025, 12, 31))\n",
        "    \n",
        "    all_symbols = list(set(CONFIG[\"symbols_to_train\"] + CONFIG[\"context_symbols\"] + [\"SPY\", \"QQQ\"]))\n",
        "    bars_df = load_all_symbols(client, all_symbols, start, end, cache=True)\n",
        "    bars_df.to_parquet(bars_path)\n",
        "\n",
        "print(f\"Bars shape: {bars_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Forecasts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_set_config = FEATURE_SET_CONFIGS[CONFIG[\"feature_set\"]]\n",
        "horizons = feature_set_config[\"horizons\"]\n",
        "print(f\"Feature set: {CONFIG['feature_set']}, Horizons: {horizons}\")\n",
        "\n",
        "all_forecast_symbols = list(set(CONFIG[\"symbols_to_train\"] + CONFIG[\"context_symbols\"]))\n",
        "print(f\"Symbols needing forecasts: {all_forecast_symbols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get decision timestamps\n",
        "first_target = CONFIG[\"symbols_to_train\"][0]\n",
        "if isinstance(full_df.index, pd.MultiIndex):\n",
        "    target_labeled = full_df.loc[first_target].copy()\n",
        "else:\n",
        "    target_labeled = full_df[full_df[\"symbol\"] == first_target].copy()\n",
        "target_labeled = target_labeled.sort_index()\n",
        "\n",
        "max_rows = CONFIG[\"max_rows_per_symbol\"]\n",
        "if max_rows and len(target_labeled) > max_rows:\n",
        "    target_labeled = target_labeled.iloc[-max_rows:]\n",
        "\n",
        "decision_timestamps = target_labeled.index.tolist()\n",
        "print(f\"Decision timestamps: {len(decision_timestamps)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate forecasts for all symbols\n",
        "all_forecasts = {}\n",
        "\n",
        "for symbol in all_forecast_symbols:\n",
        "    print(f\"\\n{'=' * 40}\")\n",
        "    print(f\"FORECASTS FOR: {symbol}\")\n",
        "    print(f\"{'=' * 40}\")\n",
        "    \n",
        "    if isinstance(bars_df.index, pd.MultiIndex):\n",
        "        symbol_bars = bars_df.loc[symbol].copy()\n",
        "    else:\n",
        "        symbol_bars = bars_df[bars_df[\"symbol\"] == symbol].copy()\n",
        "    symbol_bars = symbol_bars.sort_index()\n",
        "    \n",
        "    ts_model_path = paths[\"models\"] / \"ts\" / symbol / f\"pred_len_{CONFIG['ts_prediction_length']}\"\n",
        "    ts_model_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    first_decision = decision_timestamps[0]\n",
        "    train_data = prepare_ts_training_data(\n",
        "        bars_df=symbol_bars,\n",
        "        symbols=[symbol],\n",
        "        train_end_timestamp=first_decision,\n",
        "        lookback_years=CONFIG[\"ts_train_lookback_years\"],\n",
        "    )\n",
        "    \n",
        "    ts_predictor = load_or_train_timeseries_predictor(\n",
        "        train_data=train_data,\n",
        "        model_path=ts_model_path,\n",
        "        prediction_length=CONFIG[\"ts_prediction_length\"],\n",
        "        presets=CONFIG[\"ts_presets\"],\n",
        "        force_retrain=CONFIG[\"force_ts_retrain\"],\n",
        "    )\n",
        "    \n",
        "    forecast_cache_path = paths[\"processed\"] / \"forecasts\" / symbol / f\"fc_{CONFIG['feature_set']}_{RUN_ID}.parquet\"\n",
        "    forecast_cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    forecasts = load_or_generate_forecasts(\n",
        "        predictor=ts_predictor,\n",
        "        bars_df=symbol_bars,\n",
        "        symbol=symbol,\n",
        "        decision_timestamps=decision_timestamps,\n",
        "        cache_path=forecast_cache_path,\n",
        "        horizons=horizons,\n",
        "        force_regenerate=CONFIG[\"force_forecast_regenerate\"],\n",
        "    )\n",
        "    \n",
        "    print(f\"Forecasts shape: {forecasts.shape}\")\n",
        "    all_forecasts[symbol] = forecasts\n",
        "\n",
        "print(\"\\nForecast generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compute Forecast Error Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute error features for each target symbol\n",
        "error_features_dfs = {}\n",
        "\n",
        "# Use horizon 1 for error computation (1-step ahead accuracy)\n",
        "error_horizon = 1\n",
        "\n",
        "for symbol in CONFIG[\"symbols_to_train\"]:\n",
        "    print(f\"\\nComputing error features for {symbol}...\")\n",
        "    \n",
        "    # Get bars for this symbol\n",
        "    if isinstance(bars_df.index, pd.MultiIndex):\n",
        "        symbol_bars = bars_df.loc[symbol].copy()\n",
        "    else:\n",
        "        symbol_bars = bars_df[bars_df[\"symbol\"] == symbol].copy()\n",
        "    symbol_bars = symbol_bars.sort_index()\n",
        "    \n",
        "    # Get forecasts\n",
        "    forecasts = all_forecasts[symbol]\n",
        "    \n",
        "    # Compute realized returns at each timestamp\n",
        "    # For timestamp t, realized return is close[t+error_horizon]/close[t] - 1\n",
        "    realized_returns = []\n",
        "    for ts in forecasts.index:\n",
        "        try:\n",
        "            ts_loc = symbol_bars.index.get_loc(ts)\n",
        "            future_loc = ts_loc + error_horizon\n",
        "            if future_loc < len(symbol_bars):\n",
        "                current_close = symbol_bars.iloc[ts_loc][\"close\"]\n",
        "                future_close = symbol_bars.iloc[future_loc][\"close\"]\n",
        "                ret = np.log(future_close / current_close)\n",
        "            else:\n",
        "                ret = np.nan\n",
        "        except (KeyError, IndexError):\n",
        "            ret = np.nan\n",
        "        realized_returns.append(ret)\n",
        "    \n",
        "    realized_returns = pd.Series(realized_returns, index=forecasts.index, name=\"realized_return\")\n",
        "    \n",
        "    # Get forecast values at this horizon\n",
        "    mu_col = f\"mu_{error_horizon}\"\n",
        "    q10_col = f\"q10_{error_horizon}\"\n",
        "    q90_col = f\"q90_{error_horizon}\"\n",
        "    \n",
        "    if mu_col not in forecasts.columns:\n",
        "        print(f\"  Warning: {mu_col} not in forecasts, skipping error features\")\n",
        "        continue\n",
        "    \n",
        "    forecast_mu = forecasts[mu_col]\n",
        "    forecast_q10 = forecasts.get(q10_col, pd.Series(np.nan, index=forecasts.index))\n",
        "    forecast_q90 = forecasts.get(q90_col, pd.Series(np.nan, index=forecasts.index))\n",
        "    \n",
        "    # Compute errors\n",
        "    error_df = compute_forecast_errors(\n",
        "        realized_returns=realized_returns,\n",
        "        forecast_mu=forecast_mu,\n",
        "        forecast_q10=forecast_q10,\n",
        "        forecast_q90=forecast_q90,\n",
        "    )\n",
        "    \n",
        "    # Compute rolling error features\n",
        "    rolling_errors = compute_rolling_error_features(\n",
        "        error_df=error_df,\n",
        "        mae_window=CONFIG[\"mae_window\"],\n",
        "        bias_window=CONFIG[\"bias_window\"],\n",
        "        coverage_window=CONFIG[\"coverage_window\"],\n",
        "    )\n",
        "    \n",
        "    # Add prefix\n",
        "    rolling_errors.columns = [f\"{CONFIG['error_prefix']}{c}\" for c in rolling_errors.columns]\n",
        "    \n",
        "    print(f\"  Error feature columns: {list(rolling_errors.columns)}\")\n",
        "    error_features_dfs[symbol] = rolling_errors\n",
        "\n",
        "print(\"\\nError feature computation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Merge All Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge all features\n",
        "merged_dfs = {}\n",
        "\n",
        "for symbol in CONFIG[\"symbols_to_train\"]:\n",
        "    print(f\"\\nMerging all features for {symbol}...\")\n",
        "    \n",
        "    # Get labeled data\n",
        "    if isinstance(full_df.index, pd.MultiIndex):\n",
        "        symbol_df = full_df.loc[symbol].copy()\n",
        "    else:\n",
        "        symbol_df = full_df[full_df[\"symbol\"] == symbol].copy()\n",
        "    symbol_df = symbol_df.sort_index()\n",
        "    \n",
        "    max_rows = CONFIG[\"max_rows_per_symbol\"]\n",
        "    if max_rows and len(symbol_df) > max_rows:\n",
        "        symbol_df = symbol_df.iloc[-max_rows:]\n",
        "    \n",
        "    # 1. Add own forecast features\n",
        "    merged = merge_forecast_features(\n",
        "        tabular_df=symbol_df,\n",
        "        forecasts_df=all_forecasts[symbol],\n",
        "        feature_set=CONFIG[\"feature_set\"],\n",
        "        prefix=CONFIG[\"forecast_prefix\"],\n",
        "    )\n",
        "    print(f\"  After forecast features: {len(merged.columns)} cols\")\n",
        "    \n",
        "    # 2. Add context features\n",
        "    context_forecasts = {ctx: all_forecasts[ctx] for ctx in CONFIG[\"context_symbols\"]}\n",
        "    merged = add_context_features(\n",
        "        df=merged,\n",
        "        context_forecasts=context_forecasts,\n",
        "        target_symbol=symbol,\n",
        "        context_symbols=CONFIG[\"context_symbols\"],\n",
        "        feature_set=CONFIG[\"feature_set\"],\n",
        "        include_relative=CONFIG[\"include_relative\"],\n",
        "    )\n",
        "    print(f\"  After context features: {len(merged.columns)} cols\")\n",
        "    \n",
        "    # 3. Add error features\n",
        "    if symbol in error_features_dfs:\n",
        "        error_features = error_features_dfs[symbol]\n",
        "        merged = merged.join(error_features, how=\"left\")\n",
        "        print(f\"  After error features: {len(merged.columns)} cols\")\n",
        "    \n",
        "    merged_dfs[symbol] = merged\n",
        "\n",
        "print(\"\\nFeature merging complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define all feature columns\n",
        "baseline_features = get_feature_columns(True)\n",
        "forecast_features = get_forecast_feature_names(CONFIG[\"feature_set\"], CONFIG[\"forecast_prefix\"])\n",
        "\n",
        "# Context features\n",
        "context_features = []\n",
        "for ctx_sym in CONFIG[\"context_symbols\"]:\n",
        "    prefix = f\"ctx_{ctx_sym.lower()}_\"\n",
        "    for feat in feature_set_config[\"features\"]:\n",
        "        for h in horizons:\n",
        "            context_features.append(f\"{prefix}{feat}_{h}\")\n",
        "\n",
        "# Relative features\n",
        "relative_features = []\n",
        "if CONFIG[\"include_relative\"]:\n",
        "    for ctx_sym in CONFIG[\"context_symbols\"]:\n",
        "        for h in horizons:\n",
        "            relative_features.append(f\"rel_{ctx_sym.lower()}_mu_{h}\")\n",
        "            relative_features.append(f\"rel_{ctx_sym.lower()}_unc_{h}\")\n",
        "\n",
        "# Error features\n",
        "error_features = get_error_feature_names(\n",
        "    CONFIG[\"error_prefix\"],\n",
        "    CONFIG[\"mae_window\"],\n",
        "    CONFIG[\"bias_window\"],\n",
        "    CONFIG[\"coverage_window\"],\n",
        ")\n",
        "\n",
        "all_feature_cols = baseline_features + forecast_features + context_features + relative_features + error_features\n",
        "\n",
        "print(f\"Feature breakdown:\")\n",
        "print(f\"  Baseline: {len(baseline_features)}\")\n",
        "print(f\"  Forecast: {len(forecast_features)}\")\n",
        "print(f\"  Context: {len(context_features)}\")\n",
        "print(f\"  Relative: {len(relative_features)}\")\n",
        "print(f\"  Error: {len(error_features)}\")\n",
        "print(f\"  TOTAL: {len(all_feature_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Tabular Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results\n",
        "all_results = {}\n",
        "\n",
        "for symbol in CONFIG[\"symbols_to_train\"]:\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"TRAINING MODEL FOR: {symbol}\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "    \n",
        "    symbol_df = merged_dfs[symbol].copy()\n",
        "    \n",
        "    # Filter to available features\n",
        "    available_features = [f for f in all_feature_cols if f in symbol_df.columns]\n",
        "    print(f\"Available features: {len(available_features)}/{len(all_feature_cols)}\")\n",
        "    \n",
        "    # Clean data\n",
        "    required_cols = available_features + [CONFIG[\"label_col\"]]\n",
        "    symbol_df_clean = symbol_df.dropna(subset=required_cols).copy()\n",
        "    \n",
        "    print(f\"Data: {len(symbol_df_clean)} rows (dropped {len(symbol_df) - len(symbol_df_clean)})\")\n",
        "    \n",
        "    symbol_df_clean = symbol_df_clean.reset_index(drop=False)\n",
        "    if \"timestamp\" not in symbol_df_clean.columns and \"index\" in symbol_df_clean.columns:\n",
        "        symbol_df_clean = symbol_df_clean.rename(columns={\"index\": \"timestamp\"})\n",
        "    \n",
        "    # Create split\n",
        "    try:\n",
        "        split = create_single_split(\n",
        "            n_samples=len(symbol_df_clean),\n",
        "            vertical_barrier_bars=CONFIG[\"vertical_barrier_bars\"],\n",
        "            embargo_bars=CONFIG[\"embargo_bars\"],\n",
        "            tune_window=CONFIG[\"tune_window\"],\n",
        "            test_window=CONFIG[\"test_window\"],\n",
        "            min_train_size=CONFIG[\"min_train_size\"],\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        continue\n",
        "    \n",
        "    is_valid = validate_split_no_leakage(split, CONFIG[\"vertical_barrier_bars\"])\n",
        "    print(f\"Split valid: {is_valid}\")\n",
        "    \n",
        "    train_df, tune_df, test_df = apply_split_to_dataframe(symbol_df_clean, split)\n",
        "    print(f\"Split: Train={len(train_df)}, Tune={len(tune_df)}, Test={len(test_df)}\")\n",
        "    \n",
        "    # Train\n",
        "    model_path = paths[\"models\"] / \"exp4\" / symbol / RUN_ID\n",
        "    model_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\nTraining with {len(available_features)} features...\")\n",
        "    \n",
        "    predictor = train_tabular_baseline(\n",
        "        train_df=train_df,\n",
        "        tune_df=tune_df,\n",
        "        feature_cols=available_features,\n",
        "        label_col=CONFIG[\"label_col\"],\n",
        "        model_path=model_path,\n",
        "        time_limit=CONFIG[\"time_limit_sec\"],\n",
        "        presets=CONFIG[\"presets\"],\n",
        "        random_seed=CONFIG[\"random_seed\"],\n",
        "        verbosity=2,\n",
        "    )\n",
        "    print(\"Training complete!\")\n",
        "    \n",
        "    # Predictions\n",
        "    predictions_df = predict_tabular(predictor, test_df, available_features)\n",
        "    predictions_df[\"actual_label\"] = test_df[CONFIG[\"label_col\"]].values\n",
        "    if \"timestamp\" in test_df.columns:\n",
        "        predictions_df[\"timestamp\"] = test_df[\"timestamp\"].values\n",
        "    \n",
        "    run_dir = paths[\"runs\"] / f\"exp4_{RUN_ID}\"\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "    predictions_path = run_dir / f\"predictions_{symbol}.parquet\"\n",
        "    predictions_df.to_parquet(predictions_path)\n",
        "    \n",
        "    # Metrics\n",
        "    metrics = compute_all_metrics(\n",
        "        y_true=test_df[CONFIG[\"label_col\"]],\n",
        "        y_pred=predictions_df[\"predicted_label\"],\n",
        "        y_train=train_df[CONFIG[\"label_col\"]],\n",
        "        y_tune=tune_df[CONFIG[\"label_col\"]],\n",
        "    )\n",
        "    \n",
        "    metrics[\"run_info\"] = {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"experiment\": \"exp4\",\n",
        "        \"symbol\": symbol,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"config\": CONFIG,\n",
        "        \"feature_count\": len(available_features),\n",
        "    }\n",
        "    \n",
        "    metrics_path = run_dir / f\"metrics_{symbol}.json\"\n",
        "    save_metrics(metrics, metrics_path)\n",
        "    \n",
        "    print_metrics_summary(metrics)\n",
        "    \n",
        "    all_results[symbol] = {\n",
        "        \"metrics\": metrics,\n",
        "        \"predictor\": predictor,\n",
        "        \"model_path\": model_path,\n",
        "    }\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"ALL SYMBOLS COMPLETE!\")\n",
        "print(f\"{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"EXPERIMENT 4 SUMMARY\")\n",
        "print(f\"{'=' * 60}\")\n",
        "\n",
        "print(f\"\\nRun ID: {RUN_ID}\")\n",
        "print(f\"Symbols: {list(all_results.keys())}\")\n",
        "print(f\"Total features: {len(all_feature_cols)}\")\n",
        "\n",
        "print(f\"\\nPerformance Summary:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Symbol':<10} {'Accuracy':>10} {'Bal Acc':>10} {'Macro F1':>10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for symbol, result in all_results.items():\n",
        "    m = result[\"metrics\"][\"classification\"]\n",
        "    print(f\"{symbol:<10} {m['accuracy']:>10.4f} {m['balanced_accuracy']:>10.4f} {m['macro_f1']:>10.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with all previous experiments\n",
        "print(\"\\nComparison with Previous Experiments:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Experiment':<12} {'Symbol':<10} {'Bal Acc':>10} {'Diff':>10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for symbol in all_results.keys():\n",
        "    curr_ba = all_results[symbol][\"metrics\"][\"classification\"][\"balanced_accuracy\"]\n",
        "    \n",
        "    for exp_name in [\"exp1\", \"exp2\", \"exp3\"]:\n",
        "        exp_runs = list((paths[\"runs\"]).glob(f\"{exp_name}_*\"))\n",
        "        if exp_runs:\n",
        "            latest_run = sorted(exp_runs)[-1]\n",
        "            prev_metrics_path = latest_run / f\"metrics_{symbol}.json\"\n",
        "            if prev_metrics_path.exists():\n",
        "                with open(prev_metrics_path) as f:\n",
        "                    prev_metrics = json.load(f)\n",
        "                prev_ba = prev_metrics[\"classification\"][\"balanced_accuracy\"]\n",
        "                diff = curr_ba - prev_ba\n",
        "                print(f\"{exp_name:<12} {symbol:<10} {prev_ba:>10.4f} {diff:>+10.4f}\")\n",
        "    \n",
        "    print(f\"{'exp4':<12} {symbol:<10} {curr_ba:>10.4f} {'(current)':>10}\")\n",
        "    print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Experiment 4 Complete!**\n",
        "\n",
        "Next: Experiment 5 runs ablation studies across feature sets and model configurations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
