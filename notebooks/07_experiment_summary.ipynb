{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment Summary: Results Across All Experiments\n",
        "\n",
        "This notebook compiles and visualizes results from all experiments (1-5) to:\n",
        "\n",
        "1. Compare performance across experiments\n",
        "2. Identify the best feature configurations\n",
        "3. Analyze feature importance\n",
        "4. Generate publication-ready summary tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone/update repository\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/mh122333/ETF-Dual-Foundation-Project-CC-Version.git\"\n",
        "REPO_DIR = \"/content/ETF-Dual-Foundation-Project-CC-Version\"\n",
        "BRANCH = \"claude/build-pipeline-sanity-exp-iVs65\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    %cd {REPO_DIR}\n",
        "    !git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}\n",
        "else:\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "    %cd {REPO_DIR}\n",
        "    !git checkout {BRANCH}\n",
        "\n",
        "print(f\"\\nOn branch: {BRANCH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/ETF-Dual-Foundation-Project-CC-Version/src')\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from etf_pipeline.utils.paths import ensure_dirs\n",
        "from etf_pipeline.experiments.results import (\n",
        "    collect_experiment_results,\n",
        "    compare_experiments,\n",
        "    create_results_summary,\n",
        "    format_results_table,\n",
        "    compute_feature_ablation_impact,\n",
        ")\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths\n",
        "paths = ensure_dirs()\n",
        "runs_dir = paths[\"runs\"]\n",
        "print(f\"Runs directory: {runs_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Collect All Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect results from all experiments\n",
        "results_df = collect_experiment_results(runs_dir)\n",
        "\n",
        "if results_df.empty:\n",
        "    print(\"No experiment results found!\")\n",
        "    print(f\"Please run experiments 1-5 first. Looking in: {runs_dir}\")\n",
        "else:\n",
        "    print(f\"Collected {len(results_df)} result records\")\n",
        "    print(f\"Experiments: {sorted(results_df['experiment_name'].unique())}\")\n",
        "    print(f\"Symbols: {sorted(results_df['symbol'].unique())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all results\n",
        "if not results_df.empty:\n",
        "    display_cols = [\"experiment_name\", \"symbol\", \"run_id\", \"accuracy\", \n",
        "                   \"balanced_accuracy\", \"macro_f1\", \"cohen_kappa\"]\n",
        "    display_cols = [c for c in display_cols if c in results_df.columns]\n",
        "    print(results_df[display_cols].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Experiment Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare experiments by balanced accuracy\n",
        "if not results_df.empty:\n",
        "    comparison = compare_experiments(\n",
        "        results_df,\n",
        "        metric=\"balanced_accuracy\",\n",
        "        group_by=\"experiment_name\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\nExperiment Comparison (Balanced Accuracy):\")\n",
        "    print(\"=\" * 60)\n",
        "    print(comparison.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare by macro F1\n",
        "if not results_df.empty:\n",
        "    comparison_f1 = compare_experiments(\n",
        "        results_df,\n",
        "        metric=\"macro_f1\",\n",
        "        group_by=\"experiment_name\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\nExperiment Comparison (Macro F1):\")\n",
        "    print(\"=\" * 60)\n",
        "    print(comparison_f1.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overall summary\n",
        "if not results_df.empty:\n",
        "    summary = create_results_summary(results_df)\n",
        "    \n",
        "    print(\"\\nOverall Summary:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Total experiments: {summary['n_experiments']}\")\n",
        "    print(f\"Total runs: {summary['n_runs']}\")\n",
        "    print(f\"Symbols: {summary['symbols']}\")\n",
        "    \n",
        "    print(\"\\nMetrics Summary:\")\n",
        "    for metric, stats in summary.get(\"metrics_summary\", {}).items():\n",
        "        print(f\"  {metric}: mean={stats['mean']:.4f}, std={stats['std']:.4f}, \"\n",
        "              f\"range=[{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
        "    \n",
        "    print(\"\\nBest Runs by Experiment:\")\n",
        "    for exp, info in summary.get(\"best_runs\", {}).items():\n",
        "        print(f\"  {exp}: {info['run_id']} ({info['symbol']}) - BA: {info['balanced_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar chart comparing experiments\n",
        "if not results_df.empty and \"balanced_accuracy\" in results_df.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    exp_means = results_df.groupby(\"experiment_name\")[\"balanced_accuracy\"].mean().sort_values(ascending=False)\n",
        "    exp_stds = results_df.groupby(\"experiment_name\")[\"balanced_accuracy\"].std()\n",
        "    \n",
        "    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(exp_means)))\n",
        "    \n",
        "    bars = plt.bar(range(len(exp_means)), exp_means.values, yerr=exp_stds[exp_means.index].values,\n",
        "                   capsize=5, color=colors, edgecolor='navy', linewidth=1.5)\n",
        "    \n",
        "    plt.xticks(range(len(exp_means)), exp_means.index, rotation=45, ha='right')\n",
        "    plt.ylabel('Balanced Accuracy')\n",
        "    plt.title('Experiment Comparison: Balanced Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, (bar, val) in enumerate(zip(bars, exp_means.values)):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(paths[\"runs\"] / \"experiment_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved to: {paths['runs'] / 'experiment_comparison.png'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-metric comparison\n",
        "if not results_df.empty:\n",
        "    metrics = [\"accuracy\", \"balanced_accuracy\", \"macro_f1\"]\n",
        "    available_metrics = [m for m in metrics if m in results_df.columns]\n",
        "    \n",
        "    if available_metrics:\n",
        "        fig, axes = plt.subplots(1, len(available_metrics), figsize=(5*len(available_metrics), 5))\n",
        "        if len(available_metrics) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for ax, metric in zip(axes, available_metrics):\n",
        "            exp_order = results_df.groupby(\"experiment_name\")[metric].mean().sort_values(ascending=False).index\n",
        "            \n",
        "            sns.boxplot(data=results_df, x=\"experiment_name\", y=metric, \n",
        "                       order=exp_order, ax=ax, palette=\"Blues_d\")\n",
        "            ax.set_title(metric.replace(\"_\", \" \").title())\n",
        "            ax.set_xlabel(\"\")\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(paths[\"runs\"] / \"metrics_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Ablation Impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute feature ablation impact relative to Exp 1 baseline\n",
        "if not results_df.empty and \"exp1\" in results_df[\"experiment_name\"].values:\n",
        "    impact_df = compute_feature_ablation_impact(\n",
        "        results_df,\n",
        "        baseline_exp=\"exp1\",\n",
        "        metric=\"balanced_accuracy\"\n",
        "    )\n",
        "    \n",
        "    if not impact_df.empty:\n",
        "        print(\"\\nFeature Ablation Impact (vs Exp 1 Baseline):\")\n",
        "        print(\"=\" * 70)\n",
        "        print(impact_df.to_string(index=False))\n",
        "        \n",
        "        # Save\n",
        "        impact_df.to_csv(paths[\"runs\"] / \"feature_ablation_impact.csv\", index=False)\n",
        "        print(f\"\\nSaved to: {paths['runs'] / 'feature_ablation_impact.csv'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize ablation impact\n",
        "if not results_df.empty and 'impact_df' in dir() and not impact_df.empty:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Group by experiment\n",
        "    exp_impact = impact_df.groupby(\"experiment\")[\"relative_change_pct\"].mean().sort_values(ascending=False)\n",
        "    \n",
        "    colors = ['green' if v > 0 else 'red' for v in exp_impact.values]\n",
        "    \n",
        "    plt.barh(range(len(exp_impact)), exp_impact.values, color=colors, alpha=0.7)\n",
        "    plt.yticks(range(len(exp_impact)), exp_impact.index)\n",
        "    plt.xlabel('Relative Improvement vs Baseline (%)')\n",
        "    plt.title('Impact of Feature Additions (vs Exp 1 Baseline)')\n",
        "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, v in enumerate(exp_impact.values):\n",
        "        plt.text(v + 0.5 if v > 0 else v - 0.5, i, f'{v:.1f}%', \n",
        "                va='center', ha='left' if v > 0 else 'right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(paths[\"runs\"] / \"ablation_impact.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate markdown summary table\n",
        "if not results_df.empty:\n",
        "    # Get latest run for each experiment\n",
        "    latest_results = results_df.sort_values(\"run_id\").groupby([\"experiment_name\", \"symbol\"]).last().reset_index()\n",
        "    \n",
        "    table_md = format_results_table(\n",
        "        latest_results,\n",
        "        metrics=[\"balanced_accuracy\", \"macro_f1\", \"cohen_kappa\"],\n",
        "        sort_by=\"balanced_accuracy\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\nMarkdown Results Table:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(table_md)\n",
        "    \n",
        "    # Save\n",
        "    with open(paths[\"runs\"] / \"results_table.md\", \"w\") as f:\n",
        "        f.write(\"# Experiment Results Summary\\n\\n\")\n",
        "        f.write(f\"Generated: {datetime.now().isoformat()}\\n\\n\")\n",
        "        f.write(table_md)\n",
        "    print(f\"\\nSaved to: {paths['runs'] / 'results_table.md'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive experiment description\n",
        "experiment_descriptions = {\n",
        "    \"exp1\": \"Baseline: Causal features only (returns, volatility, ATR)\",\n",
        "    \"exp2\": \"Baseline + Time-series forecast features (Chronos)\",\n",
        "    \"exp3\": \"Baseline + Forecast + Context features (SPY/QQQ)\",\n",
        "    \"exp4\": \"Full feature set with forecast error monitoring\",\n",
        "    \"exp5\": \"Ablation studies across configurations\",\n",
        "}\n",
        "\n",
        "print(\"\\nExperiment Descriptions:\")\n",
        "print(\"=\" * 60)\n",
        "for exp, desc in experiment_descriptions.items():\n",
        "    print(f\"{exp}: {desc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary report\n",
        "if not results_df.empty:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINAL EXPERIMENT SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Best overall\n",
        "    best_idx = results_df[\"balanced_accuracy\"].idxmax()\n",
        "    best = results_df.loc[best_idx]\n",
        "    \n",
        "    print(f\"\\nBest Overall Configuration:\")\n",
        "    print(f\"  Experiment: {best['experiment_name']}\")\n",
        "    print(f\"  Symbol: {best['symbol']}\")\n",
        "    print(f\"  Run ID: {best['run_id']}\")\n",
        "    print(f\"  Balanced Accuracy: {best['balanced_accuracy']:.4f}\")\n",
        "    if 'macro_f1' in best:\n",
        "        print(f\"  Macro F1: {best['macro_f1']:.4f}\")\n",
        "    \n",
        "    # Progressive improvement\n",
        "    print(\"\\nProgressive Improvement Across Experiments:\")\n",
        "    exp_order = [\"exp1\", \"exp2\", \"exp3\", \"exp4\"]\n",
        "    prev_ba = None\n",
        "    for exp in exp_order:\n",
        "        exp_results = results_df[results_df[\"experiment_name\"] == exp]\n",
        "        if not exp_results.empty:\n",
        "            ba = exp_results[\"balanced_accuracy\"].mean()\n",
        "            if prev_ba is not None:\n",
        "                diff = ba - prev_ba\n",
        "                print(f\"  {exp}: {ba:.4f} (change: {diff:+.4f})\")\n",
        "            else:\n",
        "                print(f\"  {exp}: {ba:.4f} (baseline)\")\n",
        "            prev_ba = ba\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all results to CSV\n",
        "if not results_df.empty:\n",
        "    results_path = paths[\"runs\"] / \"all_experiment_results.csv\"\n",
        "    results_df.to_csv(results_path, index=False)\n",
        "    print(f\"All results saved to: {results_path}\")\n",
        "    \n",
        "    # Save summary JSON\n",
        "    summary_path = paths[\"runs\"] / \"experiment_summary.json\"\n",
        "    summary_data = {\n",
        "        \"generated_at\": datetime.now().isoformat(),\n",
        "        \"n_experiments\": int(results_df[\"experiment_name\"].nunique()),\n",
        "        \"n_runs\": len(results_df),\n",
        "        \"experiments\": sorted(results_df[\"experiment_name\"].unique().tolist()),\n",
        "        \"symbols\": sorted(results_df[\"symbol\"].unique().tolist()),\n",
        "        \"best_config\": {\n",
        "            \"experiment\": best[\"experiment_name\"],\n",
        "            \"symbol\": best[\"symbol\"],\n",
        "            \"run_id\": best[\"run_id\"],\n",
        "            \"balanced_accuracy\": float(best[\"balanced_accuracy\"]),\n",
        "        },\n",
        "        \"experiment_means\": results_df.groupby(\"experiment_name\")[\"balanced_accuracy\"].mean().to_dict(),\n",
        "    }\n",
        "    \n",
        "    with open(summary_path, \"w\") as f:\n",
        "        json.dump(summary_data, f, indent=2)\n",
        "    print(f\"Summary saved to: {summary_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Experiment Summary Complete!**\n",
        "\n",
        "All results have been compiled, visualized, and saved to Google Drive."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
