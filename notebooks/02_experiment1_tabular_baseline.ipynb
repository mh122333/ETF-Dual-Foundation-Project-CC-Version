{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Tabular Baseline Model\n",
    "\n",
    "This notebook trains and evaluates an AutoGluon TabularPredictor using baseline causal features\n",
    "to predict triple-barrier labels {-1, 0, +1}.\n",
    "\n",
    "**Key Features:**\n",
    "- Time-series-correct evaluation with purging + embargo\n",
    "- No future leakage from overlapping label horizons\n",
    "- Comprehensive metrics and baselines\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run Experiment 0 first to generate labeled dataset, OR this notebook will regenerate it\n",
    "- Colab A100 recommended for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "All configurable parameters are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Symbols to train on\n",
    "    \"symbols_to_train\": [\"SPY\"],  # Can expand to [\"SPY\", \"QQQ\", \"IWM\", \"AAPL\", \"MSFT\"]\n",
    "    \n",
    "    # Data limits\n",
    "    \"max_rows_per_symbol\": 6500,  # Max rows to use (set to None for all)\n",
    "    \n",
    "    # Label parameters (must match Experiment 0)\n",
    "    \"label_col\": \"label\",\n",
    "    \"vertical_barrier_bars\": 26,  # N - label horizon\n",
    "    \n",
    "    # Split parameters\n",
    "    \"embargo_bars\": 26,  # Additional embargo after purge\n",
    "    \"tune_window\": 260,  # ~1 month of 30-min bars\n",
    "    \"test_window\": 520,  # ~2 months of 30-min bars\n",
    "    \"min_train_size\": 2000,\n",
    "    \n",
    "    # AutoGluon parameters\n",
    "    \"time_limit_sec\": 1200,  # 20 minutes (adjust based on compute)\n",
    "    \"presets\": \"best_quality\",  # Options: \"medium_quality\", \"high_quality\", \"best_quality\"\n",
    "    \n",
    "    # Feature parameters\n",
    "    \"include_volume_zscore\": True,\n",
    "    \n",
    "    # Reproducibility\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Force regenerate data even if cached\n",
    "    \"force_data_refresh\": False,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "# Try installing AutoGluon with tabarena extras first\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_autogluon():\n",
    "    \"\"\"Install AutoGluon with fallback options.\"\"\"\n",
    "    # First try: tabarena extras for extreme preset\n",
    "    print(\"Attempting to install autogluon.tabular[tabarena]...\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", \"autogluon.tabular[tabarena]\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(\"tabarena install failed, falling back to standard install...\")\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", \"autogluon.tabular[all]\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(\"Full install failed, trying minimal install...\")\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", \"autogluon.tabular\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "    \n",
    "    return result.returncode == 0\n",
    "\n",
    "# Install AutoGluon\n",
    "autogluon_installed = install_autogluon()\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q pandas numpy pyarrow scikit-learn pytz alpaca-py\n",
    "\n",
    "print(\"\\nInstallation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone/update repository\nimport os\n\nREPO_URL = \"https://github.com/mh122333/ETF-Dual-Foundation-Project-CC-Version.git\"\nREPO_DIR = \"/content/ETF-Dual-Foundation-Project-CC-Version\"\nBRANCH = \"claude/build-pipeline-sanity-exp-iVs65\"  # Branch with Experiment 1 code\n\nif os.path.exists(REPO_DIR):\n    print(\"Repository exists, updating...\")\n    %cd {REPO_DIR}\n    !git fetch origin\n    !git checkout {BRANCH}\n    !git pull origin {BRANCH}\nelse:\n    print(\"Cloning repository...\")\n    !git clone {REPO_URL} {REPO_DIR}\n    %cd {REPO_DIR}\n    !git checkout {BRANCH}\n\nprint(f\"\\nOn branch: {BRANCH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path and set random seeds\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '/content/ETF-Dual-Foundation-Project-CC-Version/src')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(CONFIG[\"random_seed\"])\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "print(f\"Random seed set to: {CONFIG['random_seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Project imports\n",
    "from etf_pipeline.utils.paths import get_drive_paths, ensure_dirs, get_labeled_dataset_path\n",
    "from etf_pipeline.splits.purged_walkforward import (\n",
    "    create_single_split,\n",
    "    apply_split_to_dataframe,\n",
    "    validate_split_no_leakage,\n",
    ")\n",
    "from etf_pipeline.models.tabular_baseline import (\n",
    "    get_feature_columns_for_training,\n",
    "    train_tabular_baseline,\n",
    "    predict_tabular,\n",
    "    run_leakage_smoke_test,\n",
    "    LABEL_LEAK_COLUMNS,\n",
    ")\n",
    "from etf_pipeline.metrics.classification import (\n",
    "    compute_all_metrics,\n",
    "    save_metrics,\n",
    "    print_metrics_summary,\n",
    "    compute_label_distribution,\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "paths = ensure_dirs()\n",
    "print(\"Output directories:\")\n",
    "for name, path in paths.items():\n",
    "    print(f\"  {name}: {path}\")\n",
    "\n",
    "# Generate run ID\n",
    "run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "symbols_str = \"_\".join(CONFIG[\"symbols_to_train\"])\n",
    "RUN_ID = f\"exp1_{symbols_str}_{run_timestamp}\"\n",
    "print(f\"\\nRun ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load or Generate Data\n",
    "\n",
    "Load the labeled dataset from Experiment 0, or regenerate if missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing labeled dataset\n",
    "labeled_dataset_path = get_labeled_dataset_path()\n",
    "print(f\"Looking for labeled dataset at: {labeled_dataset_path}\")\n",
    "\n",
    "if labeled_dataset_path.exists() and not CONFIG[\"force_data_refresh\"]:\n",
    "    print(\"\\nLoading existing labeled dataset...\")\n",
    "    full_df = pd.read_parquet(labeled_dataset_path)\n",
    "    print(f\"Loaded {len(full_df)} rows\")\n",
    "else:\n",
    "    print(\"\\nLabeled dataset not found. Running Experiment 0 pipeline...\")\n",
    "    \n",
    "    # Import Experiment 0 components\n",
    "    from google.colab import userdata\n",
    "    from alpaca.data.historical import StockHistoricalDataClient\n",
    "    import pytz\n",
    "    \n",
    "    from etf_pipeline.data.alpaca import load_all_symbols\n",
    "    from etf_pipeline.labels.triple_barrier import compute_labels_multi\n",
    "    from etf_pipeline.features.baseline import compute_baseline_features_multi\n",
    "    \n",
    "    # Initialize Alpaca client\n",
    "    api_key = userdata.get(\"PAPER_KEY\")\n",
    "    api_secret = userdata.get(\"PAPER_SEC\")\n",
    "    client = StockHistoricalDataClient(api_key, api_secret)\n",
    "    \n",
    "    # Fetch data (last ~18 months)\n",
    "    eastern = pytz.timezone(\"US/Eastern\")\n",
    "    start = eastern.localize(datetime(2024, 7, 1))\n",
    "    end = eastern.localize(datetime(2025, 12, 31))\n",
    "    \n",
    "    all_symbols = [\"SPY\", \"QQQ\", \"IWM\", \"AAPL\", \"MSFT\"]\n",
    "    \n",
    "    print(f\"Fetching bars for {all_symbols}...\")\n",
    "    bars_df = load_all_symbols(client, all_symbols, start, end, cache=True)\n",
    "    \n",
    "    if bars_df.empty:\n",
    "        raise ValueError(\"No data fetched! Check Alpaca API credentials.\")\n",
    "    \n",
    "    print(f\"Fetched {len(bars_df)} bars\")\n",
    "    \n",
    "    # Compute labels\n",
    "    print(\"\\nComputing triple-barrier labels...\")\n",
    "    labeled_df = compute_labels_multi(\n",
    "        bars_df,\n",
    "        atr_window=14,\n",
    "        k_up=2.0,\n",
    "        k_dn=1.0,\n",
    "        n_bars=CONFIG[\"vertical_barrier_bars\"],\n",
    "    )\n",
    "    \n",
    "    # Compute features\n",
    "    print(\"Computing baseline features...\")\n",
    "    full_df = compute_baseline_features_multi(\n",
    "        labeled_df,\n",
    "        vol_window=20,\n",
    "        vol_zscore_window=50,\n",
    "        include_volume_zscore=CONFIG[\"include_volume_zscore\"],\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    full_df.to_parquet(labeled_dataset_path)\n",
    "    print(f\"Saved labeled dataset to: {labeled_dataset_path}\")\n",
    "\n",
    "print(f\"\\nDataset shape: {full_df.shape}\")\n",
    "print(f\"Columns: {list(full_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to symbols we want to train on\n",
    "symbols_to_train = CONFIG[\"symbols_to_train\"]\n",
    "\n",
    "# Get available symbols\n",
    "if isinstance(full_df.index, pd.MultiIndex):\n",
    "    available_symbols = full_df.index.get_level_values(\"symbol\").unique().tolist()\n",
    "else:\n",
    "    available_symbols = full_df[\"symbol\"].unique().tolist()\n",
    "\n",
    "print(f\"Available symbols: {available_symbols}\")\n",
    "print(f\"Symbols to train: {symbols_to_train}\")\n",
    "\n",
    "# Validate\n",
    "missing = set(symbols_to_train) - set(available_symbols)\n",
    "if missing:\n",
    "    raise ValueError(f\"Symbols not found in data: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Model Per Symbol\n",
    "\n",
    "For each symbol:\n",
    "1. Create purged + embargoed split\n",
    "2. Train AutoGluon TabularPredictor\n",
    "3. Evaluate on test set\n",
    "4. Save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for all symbols\n",
    "all_results = {}\n",
    "\n",
    "for symbol in symbols_to_train:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"TRAINING MODEL FOR: {symbol}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 2.1 Extract symbol data\n",
    "    # --------------------------------------------------------\n",
    "    if isinstance(full_df.index, pd.MultiIndex):\n",
    "        symbol_df = full_df.loc[symbol].copy()\n",
    "    else:\n",
    "        symbol_df = full_df[full_df[\"symbol\"] == symbol].copy()\n",
    "    \n",
    "    # Sort by time\n",
    "    symbol_df = symbol_df.sort_index()\n",
    "    \n",
    "    # Apply row limit if specified\n",
    "    max_rows = CONFIG[\"max_rows_per_symbol\"]\n",
    "    if max_rows and len(symbol_df) > max_rows:\n",
    "        # Take most recent data\n",
    "        symbol_df = symbol_df.iloc[-max_rows:]\n",
    "        print(f\"Limited to last {max_rows} rows\")\n",
    "    \n",
    "    # Get feature columns (exclude label leak columns)\n",
    "    feature_cols = get_feature_columns_for_training(\n",
    "        symbol_df, \n",
    "        include_volume_zscore=CONFIG[\"include_volume_zscore\"]\n",
    "    )\n",
    "    print(f\"\\nFeature columns: {feature_cols}\")\n",
    "    \n",
    "    # Drop rows with NaN in features or labels\n",
    "    required_cols = feature_cols + [CONFIG[\"label_col\"]]\n",
    "    symbol_df_clean = symbol_df.dropna(subset=required_cols).copy()\n",
    "    \n",
    "    print(f\"\\nData after cleaning:\")\n",
    "    print(f\"  Original rows: {len(symbol_df)}\")\n",
    "    print(f\"  Clean rows: {len(symbol_df_clean)}\")\n",
    "    print(f\"  Dropped: {len(symbol_df) - len(symbol_df_clean)}\")\n",
    "    \n",
    "    # Reset index for proper slicing\n",
    "    symbol_df_clean = symbol_df_clean.reset_index(drop=False)\n",
    "    if \"timestamp\" not in symbol_df_clean.columns and \"index\" in symbol_df_clean.columns:\n",
    "        symbol_df_clean = symbol_df_clean.rename(columns={\"index\": \"timestamp\"})\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 2.2 Create purged + embargoed split\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\nCreating time-series split with purging + embargo...\")\n",
    "    \n",
    "    try:\n",
    "        split = create_single_split(\n",
    "            n_samples=len(symbol_df_clean),\n",
    "            vertical_barrier_bars=CONFIG[\"vertical_barrier_bars\"],\n",
    "            embargo_bars=CONFIG[\"embargo_bars\"],\n",
    "            tune_window=CONFIG[\"tune_window\"],\n",
    "            test_window=CONFIG[\"test_window\"],\n",
    "            min_train_size=CONFIG[\"min_train_size\"],\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(\"Skipping this symbol due to insufficient data.\")\n",
    "        continue\n",
    "    \n",
    "    # Validate no leakage\n",
    "    is_valid = validate_split_no_leakage(split, CONFIG[\"vertical_barrier_bars\"])\n",
    "    print(f\"  Split valid (no leakage): {is_valid}\")\n",
    "    if not is_valid:\n",
    "        raise ValueError(\"Split validation failed! Possible label leakage.\")\n",
    "    \n",
    "    # Apply split\n",
    "    train_df, tune_df, test_df = apply_split_to_dataframe(symbol_df_clean, split)\n",
    "    \n",
    "    print(f\"\\nSplit sizes:\")\n",
    "    print(f\"  Train: {len(train_df)} rows (indices {split.train_start}-{split.train_end})\")\n",
    "    print(f\"  Tune:  {len(tune_df)} rows (indices {split.tune_start}-{split.tune_end})\")\n",
    "    print(f\"  Test:  {len(test_df)} rows (indices {split.test_start}-{split.test_end})\")\n",
    "    \n",
    "    # Date ranges\n",
    "    if \"timestamp\" in train_df.columns:\n",
    "        print(f\"\\nDate ranges:\")\n",
    "        print(f\"  Train: {train_df['timestamp'].min()} to {train_df['timestamp'].max()}\")\n",
    "        print(f\"  Tune:  {tune_df['timestamp'].min()} to {tune_df['timestamp'].max()}\")\n",
    "        print(f\"  Test:  {test_df['timestamp'].min()} to {test_df['timestamp'].max()}\")\n",
    "    \n",
    "    # Label distributions\n",
    "    print(\"\\nLabel distributions:\")\n",
    "    for name, df in [(\"Train\", train_df), (\"Tune\", tune_df), (\"Test\", test_df)]:\n",
    "        dist = df[CONFIG[\"label_col\"]].value_counts(normalize=True).sort_index() * 100\n",
    "        print(f\"  {name}: \" + \", \".join([f\"{k}: {v:.1f}%\" for k, v in dist.items()]))\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 2.3 Train AutoGluon model\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"TRAINING AUTOGLUON MODEL\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Model save path\n",
    "    model_path = paths[\"models\"] / \"exp1\" / symbol / RUN_ID\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\\nModel will be saved to: {model_path}\")\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining with presets='{CONFIG['presets']}', time_limit={CONFIG['time_limit_sec']}s...\")\n",
    "    print(\"This may take a while...\\n\")\n",
    "    \n",
    "    try:\n",
    "        predictor = train_tabular_baseline(\n",
    "            train_df=train_df,\n",
    "            tune_df=tune_df,\n",
    "            feature_cols=feature_cols,\n",
    "            label_col=CONFIG[\"label_col\"],\n",
    "            model_path=model_path,\n",
    "            time_limit=CONFIG[\"time_limit_sec\"],\n",
    "            presets=CONFIG[\"presets\"],\n",
    "            random_seed=CONFIG[\"random_seed\"],\n",
    "            verbosity=2,\n",
    "        )\n",
    "        print(\"\\nTraining complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during training: {e}\")\n",
    "        print(\"Trying fallback with simpler presets...\")\n",
    "        \n",
    "        # Fallback to simpler preset\n",
    "        predictor = train_tabular_baseline(\n",
    "            train_df=train_df,\n",
    "            tune_df=tune_df,\n",
    "            feature_cols=feature_cols,\n",
    "            label_col=CONFIG[\"label_col\"],\n",
    "            model_path=model_path,\n",
    "            time_limit=600,  # Shorter time\n",
    "            presets=\"medium_quality\",  # Simpler preset\n",
    "            random_seed=CONFIG[\"random_seed\"],\n",
    "            verbosity=2,\n",
    "        )\n",
    "        print(\"\\nFallback training complete!\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 2.4 Generate predictions on test set\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\nGenerating predictions on test set...\")\n",
    "    predictions_df = predict_tabular(predictor, test_df, feature_cols)\n",
    "    \n",
    "    # Add actual labels\n",
    "    predictions_df[\"actual_label\"] = test_df[CONFIG[\"label_col\"]].values\n",
    "    \n",
    "    # Add timestamp if available\n",
    "    if \"timestamp\" in test_df.columns:\n",
    "        predictions_df[\"timestamp\"] = test_df[\"timestamp\"].values\n",
    "    \n",
    "    # Save predictions\n",
    "    run_dir = paths[\"runs\"] / f\"exp1_{RUN_ID}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    predictions_path = run_dir / f\"predictions_{symbol}.parquet\"\n",
    "    predictions_df.to_parquet(predictions_path)\n",
    "    print(f\"Predictions saved to: {predictions_path}\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 2.5 Compute metrics\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\nComputing metrics...\")\n",
    "    \n",
    "    y_true = test_df[CONFIG[\"label_col\"]]\n",
    "    y_pred = predictions_df[\"predicted_label\"]\n",
    "    y_train = train_df[CONFIG[\"label_col\"]]\n",
    "    y_tune = tune_df[CONFIG[\"label_col\"]]\n",
    "    \n",
    "    metrics = compute_all_metrics(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        y_train=y_train,\n",
    "        y_tune=y_tune,\n",
    "    )\n",
    "    \n",
    "    # Add run metadata\n",
    "    metrics[\"run_info\"] = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"symbol\": symbol,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"config\": CONFIG,\n",
    "        \"split_info\": split.to_dict(),\n",
    "    }\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 2.6 Leakage smoke test\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\nRunning leakage smoke test...\")\n",
    "    leakage_results = run_leakage_smoke_test(\n",
    "        train_df=train_df,\n",
    "        test_df=test_df,\n",
    "        feature_cols=feature_cols,\n",
    "        label_col=CONFIG[\"label_col\"],\n",
    "        random_seed=CONFIG[\"random_seed\"],\n",
    "    )\n",
    "    metrics[\"leakage_test\"] = leakage_results\n",
    "    print(f\"  {leakage_results.get('interpretation', 'N/A')}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = run_dir / f\"metrics_{symbol}.json\"\n",
    "    save_metrics(metrics, metrics_path)\n",
    "    print(f\"\\nMetrics saved to: {metrics_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print_metrics_summary(metrics)\n",
    "    \n",
    "    # Store results\n",
    "    all_results[symbol] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"predictor\": predictor,\n",
    "        \"predictions_path\": predictions_path,\n",
    "        \"metrics_path\": metrics_path,\n",
    "        \"model_path\": model_path,\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL SYMBOLS COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overall summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 1 SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nRun ID: {RUN_ID}\")\n",
    "print(f\"Symbols trained: {list(all_results.keys())}\")\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Symbol':<10} {'Accuracy':>10} {'Bal Acc':>10} {'Macro F1':>10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for symbol, result in all_results.items():\n",
    "    m = result[\"metrics\"][\"classification\"]\n",
    "    print(f\"{symbol:<10} {m['accuracy']:>10.4f} {m['balanced_accuracy']:>10.4f} {m['macro_f1']:>10.4f}\")\n",
    "\n",
    "print(\"\\nArtifacts saved to:\")\n",
    "print(f\"  Runs: {paths['runs'] / f'exp1_{RUN_ID}'}\")\n",
    "print(f\"  Models: {paths['models'] / 'exp1'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved artifacts\n",
    "print(\"\\nSaved artifacts:\")\n",
    "run_dir = paths[\"runs\"] / f\"exp1_{RUN_ID}\"\n",
    "\n",
    "if run_dir.exists():\n",
    "    for f in sorted(run_dir.glob(\"*\")):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\nModel directories:\")\n",
    "model_base = paths[\"models\"] / \"exp1\"\n",
    "if model_base.exists():\n",
    "    for symbol_dir in sorted(model_base.glob(\"*\")):\n",
    "        if symbol_dir.is_dir():\n",
    "            print(f\"  {symbol_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional: Inspect Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show AutoGluon leaderboard for first symbol\n",
    "if all_results:\n",
    "    first_symbol = list(all_results.keys())[0]\n",
    "    predictor = all_results[first_symbol][\"predictor\"]\n",
    "    \n",
    "    print(f\"\\nAutoGluon Leaderboard for {first_symbol}:\")\n",
    "    try:\n",
    "        leaderboard = predictor.leaderboard(silent=True)\n",
    "        print(leaderboard.to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get leaderboard: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature importance (if available)\n",
    "if all_results:\n",
    "    first_symbol = list(all_results.keys())[0]\n",
    "    predictor = all_results[first_symbol][\"predictor\"]\n",
    "    \n",
    "    print(f\"\\nFeature Importance for {first_symbol}:\")\n",
    "    try:\n",
    "        importance = predictor.feature_importance(\n",
    "            test_df[feature_cols + [CONFIG[\"label_col\"]]],\n",
    "            silent=True\n",
    "        )\n",
    "        print(importance.to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Experiment 1 Complete!**\n",
    "\n",
    "Next steps (Experiment 2+):\n",
    "- Add time-series forecasting features (Chronos/TimeSeriesPredictor)\n",
    "- Implement more sophisticated feature engineering\n",
    "- Multi-fold cross-validation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}