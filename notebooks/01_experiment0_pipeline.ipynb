{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 0: Pipeline Sanity Check\n",
    "\n",
    "This notebook implements the full Experiment 0 pipeline:\n",
    "\n",
    "1. Download and cache 30-minute bars from Alpaca\n",
    "2. Compute ATR and triple-barrier labels\n",
    "3. Generate baseline features\n",
    "4. Run sanity checks and leakage smoke test\n",
    "5. Save artifacts to Google Drive\n",
    "\n",
    "**Prerequisites:** Run `00_setup_colab.ipynb` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%%capture\n",
    "!pip install alpaca-py pandas numpy pyarrow scikit-learn pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone/update repository\nimport os\n\nREPO_URL = \"https://github.com/mh122333/ETF-Dual-Foundation-Project-CC-Version.git\"\nREPO_DIR = \"/content/ETF-Dual-Foundation-Project-CC-Version\"\nBRANCH = \"claude/build-pipeline-sanity-exp-iVs65\"  # Branch with latest code\n\nif os.path.exists(REPO_DIR):\n    print(\"Repository exists, updating...\")\n    %cd {REPO_DIR}\n    !git fetch origin\n    !git checkout {BRANCH}\n    !git pull origin {BRANCH}\nelse:\n    print(\"Cloning repository...\")\n    !git clone {REPO_URL} {REPO_DIR}\n    %cd {REPO_DIR}\n    !git checkout {BRANCH}\n\nprint(f\"\\nOn branch: {BRANCH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/ETF-Dual-Foundation-Project-CC-Version/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "from google.colab import userdata\n",
    "from alpaca.data.historical import StockHistoricalDataClient\n",
    "\n",
    "# Project imports\n",
    "from etf_pipeline.data.alpaca import load_all_symbols\n",
    "from etf_pipeline.labels.triple_barrier import compute_labels_multi\n",
    "from etf_pipeline.features.baseline import compute_baseline_features_multi, get_feature_columns\n",
    "from etf_pipeline.sanity.checks import run_sanity_checks, save_summary\n",
    "from etf_pipeline.utils.paths import ensure_dirs, get_labeled_dataset_path, get_summary_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration (all configurable parameters)\n",
    "CONFIG = {\n",
    "    # Symbols to fetch\n",
    "    \"symbols\": [\"SPY\", \"QQQ\", \"IWM\", \"AAPL\", \"MSFT\"],\n",
    "    \n",
    "    # Date range: last ~18 months\n",
    "    \"start_date\": \"2024-07-01\",\n",
    "    \"end_date\": \"2025-12-31\",\n",
    "    \n",
    "    # ATR parameters\n",
    "    \"atr_window\": 14,\n",
    "    \n",
    "    # Triple-barrier parameters\n",
    "    \"k_up\": 2.0,  # Take-profit multiplier\n",
    "    \"k_dn\": 1.0,  # Stop-loss multiplier\n",
    "    \"n_bars\": 26,  # Vertical barrier (bars)\n",
    "    \n",
    "    # Feature parameters\n",
    "    \"vol_window\": 20,\n",
    "    \"vol_zscore_window\": 50,\n",
    "    \"include_volume_zscore\": True,\n",
    "    \n",
    "    # Cache settings\n",
    "    \"force_refresh\": False,  # Set True to re-fetch data\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "paths = ensure_dirs()\n",
    "print(\"Output directories:\")\n",
    "for name, path in paths.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Alpaca client\n",
    "# NEVER print these keys!\n",
    "api_key = userdata.get(\"PAPER_KEY\")\n",
    "api_secret = userdata.get(\"PAPER_SEC\")\n",
    "\n",
    "client = StockHistoricalDataClient(api_key, api_secret)\n",
    "print(\"Alpaca client initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates\n",
    "eastern = pytz.timezone(\"US/Eastern\")\n",
    "start = eastern.localize(datetime.strptime(CONFIG[\"start_date\"], \"%Y-%m-%d\"))\n",
    "end = eastern.localize(datetime.strptime(CONFIG[\"end_date\"], \"%Y-%m-%d\"))\n",
    "\n",
    "print(f\"Fetching data from {start} to {end}\")\n",
    "print(f\"Symbols: {CONFIG['symbols']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fetch bars for all symbols\nprint(\"\\nFetching 30-minute bars...\")\nbars_df = load_all_symbols(\n    client=client,\n    symbols=CONFIG[\"symbols\"],\n    start=start,\n    end=end,\n    cache=True,\n    force_refresh=CONFIG[\"force_refresh\"],\n)\n\nprint(f\"\\nFetched {len(bars_df)} total bars\")\nprint(f\"Shape: {bars_df.shape}\")\n\n# Check if we got any data\nif bars_df.empty:\n    raise ValueError(\n        \"No data fetched! Please check:\\n\"\n        \"  1. Your Alpaca API keys are valid (PAPER_KEY, PAPER_SEC)\\n\"\n        \"  2. The date range is valid and in the past\\n\"\n        \"  3. Your Alpaca account has market data access\\n\"\n        \"  4. Try setting force_refresh=True in CONFIG\"\n    )\n\nprint(f\"\\nBars per symbol:\")\nprint(bars_df.groupby(level=\"symbol\").size())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "print(\"Sample data:\")\n",
    "bars_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Triple-Barrier Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing triple-barrier labels...\")\n",
    "print(f\"  ATR window: {CONFIG['atr_window']}\")\n",
    "print(f\"  k_up (TP): {CONFIG['k_up']}\")\n",
    "print(f\"  k_dn (SL): {CONFIG['k_dn']}\")\n",
    "print(f\"  n_bars (vertical): {CONFIG['n_bars']}\")\n",
    "\n",
    "labeled_df = compute_labels_multi(\n",
    "    bars_df,\n",
    "    atr_window=CONFIG[\"atr_window\"],\n",
    "    k_up=CONFIG[\"k_up\"],\n",
    "    k_dn=CONFIG[\"k_dn\"],\n",
    "    n_bars=CONFIG[\"n_bars\"],\n",
    ")\n",
    "\n",
    "print(f\"\\nLabeled {len(labeled_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "print(\"Label distribution (before dropping NaN):\")\n",
    "print(labeled_df[\"label\"].value_counts().sort_index())\n",
    "print(\"\\n\")\n",
    "print(labeled_df[\"first_hit\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing baseline features...\")\n",
    "\n",
    "featured_df = compute_baseline_features_multi(\n",
    "    labeled_df,\n",
    "    vol_window=CONFIG[\"vol_window\"],\n",
    "    vol_zscore_window=CONFIG[\"vol_zscore_window\"],\n",
    "    include_volume_zscore=CONFIG[\"include_volume_zscore\"],\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures computed. Shape: {featured_df.shape}\")\n",
    "print(f\"\\nColumns: {list(featured_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature columns\n",
    "feature_cols = get_feature_columns(CONFIG[\"include_volume_zscore\"])\n",
    "print(\"Feature columns:\")\n",
    "for col in feature_cols:\n",
    "    if col in featured_df.columns:\n",
    "        print(f\"  {col}: {featured_df[col].notna().sum()} non-null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Dataset\n",
    "\n",
    "Drop rows with NaN in features or labels (typically first few rows due to rolling windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns needed for clean dataset\n",
    "required_cols = feature_cols + [\"label\"]\n",
    "available_required = [c for c in required_cols if c in featured_df.columns]\n",
    "\n",
    "# Count NaN before cleaning\n",
    "print(\"NaN counts before cleaning:\")\n",
    "for col in available_required:\n",
    "    nan_count = featured_df[col].isna().sum()\n",
    "    print(f\"  {col}: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN in required columns\n",
    "clean_df = featured_df.dropna(subset=available_required).copy()\n",
    "\n",
    "print(f\"\\nRows before cleaning: {len(featured_df)}\")\n",
    "print(f\"Rows after cleaning: {len(clean_df)}\")\n",
    "print(f\"Rows dropped: {len(featured_df) - len(clean_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no NaN in clean dataset\n",
    "print(\"\\nNaN counts after cleaning:\")\n",
    "for col in available_required:\n",
    "    nan_count = clean_df[col].isna().sum()\n",
    "    print(f\"  {col}: {nan_count}\")\n",
    "    assert nan_count == 0, f\"NaN found in {col}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running sanity checks...\\n\")\n",
    "\n",
    "results = run_sanity_checks(\n",
    "    clean_df,\n",
    "    feature_cols=feature_cols,\n",
    "    label_col=\"label\",\n",
    "    config=CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display label distribution\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LABEL DISTRIBUTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nOverall:\")\n",
    "for label, count in results[\"label_distribution\"][\"overall\"][\"counts\"].items():\n",
    "    pct = results[\"label_distribution\"][\"overall\"][\"percentages\"][label]\n",
    "    label_name = {-1: \"SL (-1)\", 0: \"Timeout (0)\", 1: \"TP (+1)\"}.get(int(label), str(label))\n",
    "    print(f\"  {label_name}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nPer Symbol:\")\n",
    "for symbol, data in results[\"label_distribution\"][\"per_symbol\"].items():\n",
    "    print(f\"\\n  {symbol}:\")\n",
    "    for label, count in data[\"counts\"].items():\n",
    "        pct = data[\"percentages\"][label]\n",
    "        label_name = {-1: \"SL\", 0: \"TO\", 1: \"TP\"}.get(int(label), str(label))\n",
    "        print(f\"    {label_name}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display leakage test results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LEAKAGE SMOKE TEST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "lt = results[\"leakage_test\"]\n",
    "if \"error\" in lt:\n",
    "    print(f\"\\nError: {lt['error']}\")\n",
    "else:\n",
    "    print(f\"\\nOriginal features accuracy: {lt['original_accuracy']:.4f}\")\n",
    "    print(f\"Shifted features accuracy:  {lt['shifted_accuracy']:.4f}\")\n",
    "    print(f\"Degradation:                {lt['degradation']:.4f}\")\n",
    "    print(f\"\\nResult: {lt['interpretation']}\")\n",
    "    print(f\"\\nTrain samples: {lt['train_samples']:,}\")\n",
    "    print(f\"Test samples:  {lt['test_samples']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labeled dataset\n",
    "dataset_path = get_labeled_dataset_path()\n",
    "clean_df.to_parquet(dataset_path)\n",
    "print(f\"Labeled dataset saved to: {dataset_path}\")\n",
    "print(f\"Size: {dataset_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary JSON\n",
    "summary_path = save_summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved artifacts\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVED ARTIFACTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "base_path = paths[\"base\"]\n",
    "print(f\"\\nBase directory: {base_path}\")\n",
    "\n",
    "# List files\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    level = root.replace(str(base_path), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        filepath = Path(root) / file\n",
    "        size_mb = filepath.stat().st_size / 1024 / 1024\n",
    "        print(f\"{subindent}{file} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Experiment 0 pipeline completed successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 0 COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Symbols: {CONFIG['symbols']}\")\n",
    "print(f\"  Date range: {CONFIG['start_date']} to {CONFIG['end_date']}\")\n",
    "print(f\"  Total rows (clean): {len(clean_df):,}\")\n",
    "print(f\"  Features: {feature_cols}\")\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "overall = results[\"label_distribution\"][\"overall\"]\n",
    "for label in [-1, 0, 1]:\n",
    "    if label in overall[\"counts\"]:\n",
    "        print(f\"  {label}: {overall['counts'][label]:,} ({overall['percentages'][label]:.1f}%)\")\n",
    "\n",
    "print(f\"\\nLeakage test: {'PASSED' if results['leakage_test'].get('passed', False) else 'CHECK RESULTS'}\")\n",
    "\n",
    "print(f\"\\nArtifacts saved to:\")\n",
    "print(f\"  {paths['base']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}