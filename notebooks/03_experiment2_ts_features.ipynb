{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 2: Baseline + Time-Series Forecast Features\n",
        "\n",
        "This notebook extends Experiment 1 by adding time-series forecast-derived features from AutoGluon TimeSeriesPredictor with Chronos models.\n",
        "\n",
        "**New Features Added:**\n",
        "- Mean forecast (mu) at multiple horizons\n",
        "- Forecast uncertainty (q90-q10 spread)\n",
        "- Forecast trend (direction indicator)\n",
        "- Position in interval (where current price sits in forecast range)\n",
        "\n",
        "**Key Design:**\n",
        "- Rolling forecasts generated causally (no future data leakage)\n",
        "- Cached to Drive for reproducibility\n",
        "- Uses same purged + embargoed splits as Exp 1\n",
        "\n",
        "**Prerequisites:**\n",
        "- Run Experiment 0/1 first to generate labeled dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Modify these parameters as needed\n",
        "# ============================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # Symbols to train on\n",
        "    \"symbols_to_train\": [\"SPY\"],\n",
        "    \n",
        "    # Data limits\n",
        "    \"max_rows_per_symbol\": 6500,\n",
        "    \n",
        "    # Label parameters (must match Experiment 0)\n",
        "    \"label_col\": \"label\",\n",
        "    \"vertical_barrier_bars\": 26,\n",
        "    \n",
        "    # Split parameters\n",
        "    \"embargo_bars\": 26,\n",
        "    \"tune_window\": 260,\n",
        "    \"test_window\": 520,\n",
        "    \"min_train_size\": 2000,\n",
        "    \n",
        "    # AutoGluon Tabular parameters\n",
        "    \"time_limit_sec\": 1200,\n",
        "    \"presets\": \"best_quality\",\n",
        "    \n",
        "    # Time Series parameters\n",
        "    \"ts_prediction_length\": 26,\n",
        "    \"ts_presets\": \"chronos_small\",  # chronos_small, chronos_base, chronos_large\n",
        "    \"ts_train_lookback_years\": 5.0,\n",
        "    \n",
        "    # Forecast feature parameters\n",
        "    \"feature_set\": \"small\",  # small, medium, large\n",
        "    \"forecast_prefix\": \"fc_\",\n",
        "    \n",
        "    # Reproducibility\n",
        "    \"random_seed\": 42,\n",
        "    \n",
        "    # Force options\n",
        "    \"force_data_refresh\": False,\n",
        "    \"force_ts_retrain\": False,\n",
        "    \"force_forecast_regenerate\": False,\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "\n",
        "# AutoGluon tabular with tabarena for best quality\n",
        "!pip install -q autogluon.tabular[tabarena] || pip install -q autogluon.tabular[all]\n",
        "\n",
        "# AutoGluon timeseries with Chronos\n",
        "!pip install -q autogluon.timeseries[chronos-openvino]\n",
        "\n",
        "# Other dependencies\n",
        "!pip install -q pandas numpy pyarrow scikit-learn pytz alpaca-py\n",
        "\n",
        "print(\"\\nInstallation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone/update repository\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/mh122333/ETF-Dual-Foundation-Project-CC-Version.git\"\n",
        "REPO_DIR = \"/content/ETF-Dual-Foundation-Project-CC-Version\"\n",
        "BRANCH = \"claude/build-pipeline-sanity-exp-iVs65\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repository exists, updating...\")\n",
        "    %cd {REPO_DIR}\n",
        "    !git fetch origin\n",
        "    !git checkout {BRANCH}\n",
        "    !git pull origin {BRANCH}\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "    %cd {REPO_DIR}\n",
        "    !git checkout {BRANCH}\n",
        "\n",
        "print(f\"\\nOn branch: {BRANCH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path and set random seeds\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "sys.path.insert(0, '/content/ETF-Dual-Foundation-Project-CC-Version/src')\n",
        "\n",
        "# Set random seeds\n",
        "random.seed(CONFIG[\"random_seed\"])\n",
        "np.random.seed(CONFIG[\"random_seed\"])\n",
        "\n",
        "print(f\"Random seed set to: {CONFIG['random_seed']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Project imports\n",
        "from etf_pipeline.utils.paths import get_drive_paths, ensure_dirs, get_labeled_dataset_path\n",
        "from etf_pipeline.splits.purged_walkforward import (\n",
        "    create_single_split,\n",
        "    apply_split_to_dataframe,\n",
        "    validate_split_no_leakage,\n",
        ")\n",
        "from etf_pipeline.models.tabular_baseline import (\n",
        "    get_feature_columns_for_training,\n",
        "    train_tabular_baseline,\n",
        "    predict_tabular,\n",
        "    LABEL_LEAK_COLUMNS,\n",
        ")\n",
        "from etf_pipeline.metrics.classification import (\n",
        "    compute_all_metrics,\n",
        "    save_metrics,\n",
        "    print_metrics_summary,\n",
        ")\n",
        "\n",
        "# Time series imports\n",
        "from etf_pipeline.timeseries.dataset import (\n",
        "    build_returns_series,\n",
        "    prepare_ts_training_data,\n",
        ")\n",
        "from etf_pipeline.timeseries.train import load_or_train_timeseries_predictor\n",
        "from etf_pipeline.timeseries.rolling_predict import load_or_generate_forecasts\n",
        "\n",
        "# Forecast feature imports\n",
        "from etf_pipeline.features.forecast_features import (\n",
        "    merge_forecast_features,\n",
        "    get_forecast_feature_names,\n",
        "    FEATURE_SET_CONFIGS,\n",
        ")\n",
        "from etf_pipeline.features.baseline import get_feature_columns\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directories\n",
        "paths = ensure_dirs()\n",
        "print(\"Output directories:\")\n",
        "for name, path in paths.items():\n",
        "    print(f\"  {name}: {path}\")\n",
        "\n",
        "# Generate run ID\n",
        "run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "symbols_str = \"_\".join(CONFIG[\"symbols_to_train\"])\n",
        "RUN_ID = f\"exp2_{symbols_str}_{run_timestamp}\"\n",
        "print(f\"\\nRun ID: {RUN_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load labeled dataset from Experiment 0/1\n",
        "labeled_dataset_path = get_labeled_dataset_path()\n",
        "print(f\"Loading labeled dataset from: {labeled_dataset_path}\")\n",
        "\n",
        "if not labeled_dataset_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Labeled dataset not found at {labeled_dataset_path}. \"\n",
        "        \"Please run Experiment 0 or 1 first.\"\n",
        "    )\n",
        "\n",
        "full_df = pd.read_parquet(labeled_dataset_path)\n",
        "print(f\"Loaded {len(full_df)} rows\")\n",
        "print(f\"Columns: {list(full_df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw bars for time series training\n",
        "# We need the full price history, not just labeled data\n",
        "bars_path = paths[\"raw\"] / \"bars_30min.parquet\"\n",
        "\n",
        "if bars_path.exists():\n",
        "    print(f\"Loading cached bars from: {bars_path}\")\n",
        "    bars_df = pd.read_parquet(bars_path)\n",
        "else:\n",
        "    print(\"Fetching bars from Alpaca...\")\n",
        "    from google.colab import userdata\n",
        "    from alpaca.data.historical import StockHistoricalDataClient\n",
        "    import pytz\n",
        "    from etf_pipeline.data.alpaca import load_all_symbols\n",
        "    \n",
        "    api_key = userdata.get(\"PAPER_KEY\")\n",
        "    api_secret = userdata.get(\"PAPER_SEC\")\n",
        "    client = StockHistoricalDataClient(api_key, api_secret)\n",
        "    \n",
        "    eastern = pytz.timezone(\"US/Eastern\")\n",
        "    start = eastern.localize(datetime(2019, 1, 1))  # 5+ years for TS training\n",
        "    end = eastern.localize(datetime(2025, 12, 31))\n",
        "    \n",
        "    all_symbols = [\"SPY\", \"QQQ\", \"IWM\", \"AAPL\", \"MSFT\"]\n",
        "    bars_df = load_all_symbols(client, all_symbols, start, end, cache=True)\n",
        "    \n",
        "    # Save for future use\n",
        "    bars_df.to_parquet(bars_path)\n",
        "    print(f\"Saved bars to: {bars_path}\")\n",
        "\n",
        "print(f\"Bars shape: {bars_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train Time Series Model and Generate Forecasts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define forecast horizons based on feature set\n",
        "feature_set_config = FEATURE_SET_CONFIGS[CONFIG[\"feature_set\"]]\n",
        "horizons = feature_set_config[\"horizons\"]\n",
        "print(f\"Feature set: {CONFIG['feature_set']}\")\n",
        "print(f\"Horizons: {horizons}\")\n",
        "print(f\"Features per horizon: {feature_set_config['features']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store forecasts for all symbols\n",
        "all_forecasts = {}\n",
        "\n",
        "for symbol in CONFIG[\"symbols_to_train\"]:\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"GENERATING FORECASTS FOR: {symbol}\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "    \n",
        "    # Get symbol bars\n",
        "    if isinstance(bars_df.index, pd.MultiIndex):\n",
        "        symbol_bars = bars_df.loc[symbol].copy()\n",
        "    else:\n",
        "        symbol_bars = bars_df[bars_df[\"symbol\"] == symbol].copy()\n",
        "    symbol_bars = symbol_bars.sort_index()\n",
        "    \n",
        "    # Get decision timestamps from labeled data\n",
        "    if isinstance(full_df.index, pd.MultiIndex):\n",
        "        symbol_labeled = full_df.loc[symbol].copy()\n",
        "    else:\n",
        "        symbol_labeled = full_df[full_df[\"symbol\"] == symbol].copy()\n",
        "    symbol_labeled = symbol_labeled.sort_index()\n",
        "    \n",
        "    # Apply row limit\n",
        "    max_rows = CONFIG[\"max_rows_per_symbol\"]\n",
        "    if max_rows and len(symbol_labeled) > max_rows:\n",
        "        symbol_labeled = symbol_labeled.iloc[-max_rows:]\n",
        "    \n",
        "    decision_timestamps = symbol_labeled.index.tolist()\n",
        "    print(f\"Decision timestamps: {len(decision_timestamps)}\")\n",
        "    print(f\"  First: {decision_timestamps[0]}\")\n",
        "    print(f\"  Last: {decision_timestamps[-1]}\")\n",
        "    \n",
        "    # Model path for TS predictor\n",
        "    ts_model_path = paths[\"models\"] / \"ts\" / symbol / f\"pred_len_{CONFIG['ts_prediction_length']}\"\n",
        "    ts_model_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Prepare TS training data (up to first decision timestamp)\n",
        "    first_decision = decision_timestamps[0]\n",
        "    train_data = prepare_ts_training_data(\n",
        "        bars_df=symbol_bars,\n",
        "        symbols=[symbol],\n",
        "        train_end_timestamp=first_decision,\n",
        "        lookback_years=CONFIG[\"ts_train_lookback_years\"],\n",
        "    )\n",
        "    print(f\"\\nTS training data shape: {train_data.shape}\")\n",
        "    \n",
        "    # Train or load TS predictor\n",
        "    print(f\"\\nTraining/loading TimeSeriesPredictor...\")\n",
        "    ts_predictor = load_or_train_timeseries_predictor(\n",
        "        train_data=train_data,\n",
        "        model_path=ts_model_path,\n",
        "        prediction_length=CONFIG[\"ts_prediction_length\"],\n",
        "        presets=CONFIG[\"ts_presets\"],\n",
        "        force_retrain=CONFIG[\"force_ts_retrain\"],\n",
        "    )\n",
        "    \n",
        "    # Generate rolling forecasts\n",
        "    print(f\"\\nGenerating rolling forecasts...\")\n",
        "    forecast_cache_path = paths[\"processed\"] / \"forecasts\" / symbol / f\"fc_{CONFIG['feature_set']}_{RUN_ID}.parquet\"\n",
        "    forecast_cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    forecasts = load_or_generate_forecasts(\n",
        "        predictor=ts_predictor,\n",
        "        bars_df=symbol_bars,\n",
        "        symbol=symbol,\n",
        "        decision_timestamps=decision_timestamps,\n",
        "        cache_path=forecast_cache_path,\n",
        "        horizons=horizons,\n",
        "        force_regenerate=CONFIG[\"force_forecast_regenerate\"],\n",
        "    )\n",
        "    \n",
        "    print(f\"Forecasts shape: {forecasts.shape}\")\n",
        "    print(f\"Forecast columns: {list(forecasts.columns)}\")\n",
        "    \n",
        "    all_forecasts[symbol] = forecasts\n",
        "\n",
        "print(\"\\nForecast generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Merge Forecast Features with Baseline Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge forecast features for each symbol\n",
        "merged_dfs = {}\n",
        "\n",
        "for symbol in CONFIG[\"symbols_to_train\"]:\n",
        "    print(f\"\\nMerging features for {symbol}...\")\n",
        "    \n",
        "    # Get labeled data\n",
        "    if isinstance(full_df.index, pd.MultiIndex):\n",
        "        symbol_df = full_df.loc[symbol].copy()\n",
        "    else:\n",
        "        symbol_df = full_df[full_df[\"symbol\"] == symbol].copy()\n",
        "    symbol_df = symbol_df.sort_index()\n",
        "    \n",
        "    # Apply row limit\n",
        "    max_rows = CONFIG[\"max_rows_per_symbol\"]\n",
        "    if max_rows and len(symbol_df) > max_rows:\n",
        "        symbol_df = symbol_df.iloc[-max_rows:]\n",
        "    \n",
        "    # Get forecasts\n",
        "    forecasts = all_forecasts[symbol]\n",
        "    \n",
        "    # Merge forecast features\n",
        "    merged = merge_forecast_features(\n",
        "        tabular_df=symbol_df,\n",
        "        forecasts_df=forecasts,\n",
        "        feature_set=CONFIG[\"feature_set\"],\n",
        "        prefix=CONFIG[\"forecast_prefix\"],\n",
        "    )\n",
        "    \n",
        "    print(f\"  Original columns: {len(symbol_df.columns)}\")\n",
        "    print(f\"  Merged columns: {len(merged.columns)}\")\n",
        "    \n",
        "    # Show new forecast columns\n",
        "    fc_cols = [c for c in merged.columns if c.startswith(CONFIG[\"forecast_prefix\"])]\n",
        "    print(f\"  Forecast feature columns: {fc_cols}\")\n",
        "    \n",
        "    merged_dfs[symbol] = merged\n",
        "\n",
        "print(\"\\nFeature merging complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define all feature columns for Exp 2\n",
        "baseline_features = get_feature_columns(CONFIG.get(\"include_volume_zscore\", True))\n",
        "forecast_features = get_forecast_feature_names(CONFIG[\"feature_set\"], CONFIG[\"forecast_prefix\"])\n",
        "\n",
        "all_feature_cols = baseline_features + forecast_features\n",
        "\n",
        "print(f\"Baseline features ({len(baseline_features)}): {baseline_features}\")\n",
        "print(f\"\\nForecast features ({len(forecast_features)}): {forecast_features}\")\n",
        "print(f\"\\nTotal features: {len(all_feature_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Tabular Model with Combined Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results\n",
        "all_results = {}\n",
        "\n",
        "for symbol in CONFIG[\"symbols_to_train\"]:\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"TRAINING MODEL FOR: {symbol}\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "    \n",
        "    # Get merged data\n",
        "    symbol_df = merged_dfs[symbol].copy()\n",
        "    \n",
        "    # Drop rows with NaN in features or labels\n",
        "    required_cols = all_feature_cols + [CONFIG[\"label_col\"]]\n",
        "    symbol_df_clean = symbol_df.dropna(subset=required_cols).copy()\n",
        "    \n",
        "    print(f\"\\nData after cleaning:\")\n",
        "    print(f\"  Original rows: {len(symbol_df)}\")\n",
        "    print(f\"  Clean rows: {len(symbol_df_clean)}\")\n",
        "    print(f\"  Dropped: {len(symbol_df) - len(symbol_df_clean)}\")\n",
        "    \n",
        "    # Reset index\n",
        "    symbol_df_clean = symbol_df_clean.reset_index(drop=False)\n",
        "    if \"timestamp\" not in symbol_df_clean.columns and \"index\" in symbol_df_clean.columns:\n",
        "        symbol_df_clean = symbol_df_clean.rename(columns={\"index\": \"timestamp\"})\n",
        "    \n",
        "    # Create split\n",
        "    print(\"\\nCreating time-series split with purging + embargo...\")\n",
        "    try:\n",
        "        split = create_single_split(\n",
        "            n_samples=len(symbol_df_clean),\n",
        "            vertical_barrier_bars=CONFIG[\"vertical_barrier_bars\"],\n",
        "            embargo_bars=CONFIG[\"embargo_bars\"],\n",
        "            tune_window=CONFIG[\"tune_window\"],\n",
        "            test_window=CONFIG[\"test_window\"],\n",
        "            min_train_size=CONFIG[\"min_train_size\"],\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        print(\"Skipping this symbol due to insufficient data.\")\n",
        "        continue\n",
        "    \n",
        "    # Validate\n",
        "    is_valid = validate_split_no_leakage(split, CONFIG[\"vertical_barrier_bars\"])\n",
        "    print(f\"  Split valid (no leakage): {is_valid}\")\n",
        "    if not is_valid:\n",
        "        raise ValueError(\"Split validation failed!\")\n",
        "    \n",
        "    # Apply split\n",
        "    train_df, tune_df, test_df = apply_split_to_dataframe(symbol_df_clean, split)\n",
        "    \n",
        "    print(f\"\\nSplit sizes:\")\n",
        "    print(f\"  Train: {len(train_df)} rows\")\n",
        "    print(f\"  Tune:  {len(tune_df)} rows\")\n",
        "    print(f\"  Test:  {len(test_df)} rows\")\n",
        "    \n",
        "    # Train model\n",
        "    print(f\"\\n{'-' * 40}\")\n",
        "    print(\"TRAINING AUTOGLUON MODEL\")\n",
        "    print(f\"{'-' * 40}\")\n",
        "    \n",
        "    model_path = paths[\"models\"] / \"exp2\" / symbol / RUN_ID\n",
        "    model_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\nFeatures used: {len(all_feature_cols)}\")\n",
        "    print(f\"Training with presets='{CONFIG['presets']}', time_limit={CONFIG['time_limit_sec']}s...\")\n",
        "    \n",
        "    predictor = train_tabular_baseline(\n",
        "        train_df=train_df,\n",
        "        tune_df=tune_df,\n",
        "        feature_cols=all_feature_cols,\n",
        "        label_col=CONFIG[\"label_col\"],\n",
        "        model_path=model_path,\n",
        "        time_limit=CONFIG[\"time_limit_sec\"],\n",
        "        presets=CONFIG[\"presets\"],\n",
        "        random_seed=CONFIG[\"random_seed\"],\n",
        "        verbosity=2,\n",
        "    )\n",
        "    print(\"\\nTraining complete!\")\n",
        "    \n",
        "    # Predictions\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "    predictions_df = predict_tabular(predictor, test_df, all_feature_cols)\n",
        "    predictions_df[\"actual_label\"] = test_df[CONFIG[\"label_col\"]].values\n",
        "    if \"timestamp\" in test_df.columns:\n",
        "        predictions_df[\"timestamp\"] = test_df[\"timestamp\"].values\n",
        "    \n",
        "    # Save predictions\n",
        "    run_dir = paths[\"runs\"] / f\"exp2_{RUN_ID}\"\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "    predictions_path = run_dir / f\"predictions_{symbol}.parquet\"\n",
        "    predictions_df.to_parquet(predictions_path)\n",
        "    print(f\"Predictions saved to: {predictions_path}\")\n",
        "    \n",
        "    # Metrics\n",
        "    print(\"\\nComputing metrics...\")\n",
        "    metrics = compute_all_metrics(\n",
        "        y_true=test_df[CONFIG[\"label_col\"]],\n",
        "        y_pred=predictions_df[\"predicted_label\"],\n",
        "        y_train=train_df[CONFIG[\"label_col\"]],\n",
        "        y_tune=tune_df[CONFIG[\"label_col\"]],\n",
        "    )\n",
        "    \n",
        "    metrics[\"run_info\"] = {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"experiment\": \"exp2\",\n",
        "        \"symbol\": symbol,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"config\": CONFIG,\n",
        "        \"feature_count\": len(all_feature_cols),\n",
        "        \"baseline_feature_count\": len(baseline_features),\n",
        "        \"forecast_feature_count\": len(forecast_features),\n",
        "    }\n",
        "    \n",
        "    # Save metrics\n",
        "    metrics_path = run_dir / f\"metrics_{symbol}.json\"\n",
        "    save_metrics(metrics, metrics_path)\n",
        "    print(f\"Metrics saved to: {metrics_path}\")\n",
        "    \n",
        "    print_metrics_summary(metrics)\n",
        "    \n",
        "    all_results[symbol] = {\n",
        "        \"metrics\": metrics,\n",
        "        \"predictor\": predictor,\n",
        "        \"predictions_path\": predictions_path,\n",
        "        \"metrics_path\": metrics_path,\n",
        "        \"model_path\": model_path,\n",
        "    }\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"ALL SYMBOLS COMPLETE!\")\n",
        "print(f\"{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print summary\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"EXPERIMENT 2 SUMMARY\")\n",
        "print(f\"{'=' * 60}\")\n",
        "\n",
        "print(f\"\\nRun ID: {RUN_ID}\")\n",
        "print(f\"Symbols trained: {list(all_results.keys())}\")\n",
        "print(f\"Feature set: {CONFIG['feature_set']}\")\n",
        "print(f\"Total features: {len(all_feature_cols)}\")\n",
        "print(f\"  - Baseline: {len(baseline_features)}\")\n",
        "print(f\"  - Forecast: {len(forecast_features)}\")\n",
        "\n",
        "print(f\"\\nPerformance Summary:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Symbol':<10} {'Accuracy':>10} {'Bal Acc':>10} {'Macro F1':>10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for symbol, result in all_results.items():\n",
        "    m = result[\"metrics\"][\"classification\"]\n",
        "    print(f\"{symbol:<10} {m['accuracy']:>10.4f} {m['balanced_accuracy']:>10.4f} {m['macro_f1']:>10.4f}\")\n",
        "\n",
        "print(f\"\\nArtifacts saved to:\")\n",
        "print(f\"  Runs: {paths['runs'] / f'exp2_{RUN_ID}'}\")\n",
        "print(f\"  Models: {paths['models'] / 'exp2'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with Exp 1 if available\n",
        "print(\"\\nComparison with Experiment 1 (if available):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "exp1_runs = list((paths[\"runs\"]).glob(\"exp1_*\"))\n",
        "if exp1_runs:\n",
        "    latest_exp1 = sorted(exp1_runs)[-1]\n",
        "    print(f\"Latest Exp 1 run: {latest_exp1.name}\")\n",
        "    \n",
        "    for symbol in all_results.keys():\n",
        "        exp1_metrics_path = latest_exp1 / f\"metrics_{symbol}.json\"\n",
        "        if exp1_metrics_path.exists():\n",
        "            with open(exp1_metrics_path) as f:\n",
        "                exp1_metrics = json.load(f)\n",
        "            \n",
        "            exp1_ba = exp1_metrics[\"classification\"][\"balanced_accuracy\"]\n",
        "            exp2_ba = all_results[symbol][\"metrics\"][\"classification\"][\"balanced_accuracy\"]\n",
        "            diff = exp2_ba - exp1_ba\n",
        "            \n",
        "            print(f\"\\n{symbol}:\")\n",
        "            print(f\"  Exp 1 Balanced Accuracy: {exp1_ba:.4f}\")\n",
        "            print(f\"  Exp 2 Balanced Accuracy: {exp2_ba:.4f}\")\n",
        "            print(f\"  Improvement: {diff:+.4f} ({100*diff/exp1_ba:+.2f}%)\")\n",
        "else:\n",
        "    print(\"No Experiment 1 runs found for comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Experiment 2 Complete!**\n",
        "\n",
        "Next: Experiment 3 adds context asset forecast features (SPY, QQQ)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
